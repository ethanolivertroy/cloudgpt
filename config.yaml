# llm-cloudpolicy-scanner Configuration

llm:
  provider: openai
  model: gpt-4
  temperature: 0.5
  max_tokens: 1000
  top_p: 1
  frequency_penalty: 0.0
  presence_penalty: 0.0
  stream: false

multi_agent:
  enabled: true  # Multi-agent analysis with CrewAI
  framework: crewai
  verbose: true  # Show detailed agent output
  agents:
    - iam_analyst
    - network_security
    - data_protection
    - compliance
    - orchestrator

obfuscation:
  enabled: true
  consistent_mapping: true  # Same sensitive value always maps to same redacted value
  audit_log: true  # Track all redactions for audit purposes
  export_audit: false  # Export audit log to file
  audit_filename: "cache/redaction_audit.json"
  # Patterns to enable per provider (empty list = all patterns)
  aws_patterns:
    - account_id
    - arn
    - iam_user
    - iam_role
    - ipv4
    - email
  azure_patterns:
    - subscription_id
    - tenant_id
    - resource_id
    - principal_id
    - ipv4
    - email
  gcp_patterns:
    - project_id
    - project_number
    - service_account
    - org_id
    - ipv4
    - email

neo4j:
  enabled: false  # Set to true when Phase 5 is implemented
  uri: bolt://localhost:7687
  username: neo4j
  password_env: NEO4J_PASSWORD
  database: cloudpolicies

output:
  formats:
    - csv
  directory: ./cache
  include_timestamp: true

scanning:
  parallel: false  # Set to true when Phase 6 is implemented
  max_workers: 5
  resume_on_failure: false

logging:
  level: INFO
  format: "[%(levelname)s] %(message)s"
  file: scan.log
  console: true
